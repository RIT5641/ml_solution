{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9f040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Users/fernandolopezcoronado/opt/anaconda3/lib/python3.9/site-packages (0.12.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/fernandolopezcoronado/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/fernandolopezcoronado/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/fernandolopezcoronado/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/fernandolopezcoronado/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/fernandolopezcoronado/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db7ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16be099d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>return</th>\n",
       "      <th>signal</th>\n",
       "      <th>market_sentiment</th>\n",
       "      <th>recession_expectation</th>\n",
       "      <th>growing_sector</th>\n",
       "      <th>investor_type</th>\n",
       "      <th>news_impact</th>\n",
       "      <th>policy_uncertainty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>424.915955</td>\n",
       "      <td>76982300</td>\n",
       "      <td>-0.005124</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>medium</td>\n",
       "      <td>health</td>\n",
       "      <td>retail</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22</th>\n",
       "      <td>426.449768</td>\n",
       "      <td>70637200</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>high</td>\n",
       "      <td>health</td>\n",
       "      <td>retail</td>\n",
       "      <td>negative</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-23</th>\n",
       "      <td>423.225800</td>\n",
       "      <td>92074500</td>\n",
       "      <td>-0.007560</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>medium</td>\n",
       "      <td>finance</td>\n",
       "      <td>institutional</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-26</th>\n",
       "      <td>421.496613</td>\n",
       "      <td>72823600</td>\n",
       "      <td>-0.004086</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>low</td>\n",
       "      <td>health</td>\n",
       "      <td>retail</td>\n",
       "      <td>negative</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-27</th>\n",
       "      <td>426.117554</td>\n",
       "      <td>72813700</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>low</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>retail</td>\n",
       "      <td>positive</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Close    Volume    return  signal market_sentiment  \\\n",
       "Date                                                                  \n",
       "2023-06-21  424.915955  76982300 -0.005124       0         negative   \n",
       "2023-06-22  426.449768  70637200  0.003610       1         positive   \n",
       "2023-06-23  423.225800  92074500 -0.007560       0         negative   \n",
       "2023-06-26  421.496613  72823600 -0.004086       0         negative   \n",
       "2023-06-27  426.117554  72813700  0.010963       1         positive   \n",
       "\n",
       "           recession_expectation growing_sector  investor_type news_impact  \\\n",
       "Date                                                                         \n",
       "2023-06-21                medium         health         retail     neutral   \n",
       "2023-06-22                  high         health         retail    negative   \n",
       "2023-06-23                medium        finance  institutional     neutral   \n",
       "2023-06-26                   low         health         retail    negative   \n",
       "2023-06-27                   low  manufacturing         retail    positive   \n",
       "\n",
       "           policy_uncertainty  \n",
       "Date                           \n",
       "2023-06-21                low  \n",
       "2023-06-22             medium  \n",
       "2023-06-23                low  \n",
       "2023-06-26                low  \n",
       "2023-06-27                low  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('SPY_dataset_project.csv', parse_dates=['Date'], index_col='Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7389f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornos\n",
    "df['return']     = df['Close'].pct_change()\n",
    "df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "\n",
    "# SMA y EWMA\n",
    "df['SMA_5']   = df['Close'].rolling(5).mean()\n",
    "df['SMA_10']  = df['Close'].rolling(10).mean()\n",
    "df['EWMA_5']  = df['Close'].ewm(span=5,  adjust=False).mean()\n",
    "df['EWMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "\n",
    "# Volatilidad a 10 días\n",
    "df['Volatility_10'] = df['Close'].rolling(10).std()\n",
    "\n",
    "# RSI (14 días)\n",
    "delta     = df['Close'].diff()\n",
    "gain      = delta.clip(lower=0)\n",
    "loss      = -delta.clip(upper=0)\n",
    "avg_gain  = gain.rolling(14).mean()\n",
    "avg_loss  = loss.rolling(14).mean()\n",
    "rs        = avg_gain / avg_loss\n",
    "df['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Bollinger Bands (20 días)\n",
    "df['SMA_20']   = df['Close'].rolling(20).mean()\n",
    "rolling_std    = df['Close'].rolling(20).std()\n",
    "df['BB_Upper'] = df['SMA_20'] + 2 * rolling_std\n",
    "df['BB_Lower'] = df['SMA_20'] - 2 * rolling_std\n",
    "\n",
    "# Cambio de volumen y momentum\n",
    "df['Volume_change'] = df['Volume'].pct_change()\n",
    "df['Momentum_5']    = df['Close'] - df['Close'].shift(5)\n",
    "\n",
    "# Eliminar filas con NaN\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53eda778",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'market_sentiment','recession_expectation','growing_sector',\n",
    "    'investor_type','news_impact','policy_uncertainty'\n",
    "]\n",
    "numerical_features = [\n",
    "    'SMA_5','SMA_10','EWMA_5','EWMA_10','Volatility_10',\n",
    "    'RSI_14','BB_Upper','BB_Lower','Volume_change','Momentum_5','log_return'\n",
    "]\n",
    "\n",
    "X = df[categorical_features + numerical_features]\n",
    "y = df['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b450b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f2582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "    ('num', StandardScaler(), numerical_features)\n",
    "])\n",
    "\n",
    "pipeline1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote',        SMOTE(random_state=42)),\n",
    "    ('classifier',   LogisticRegression(solver='liblinear', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ca9aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['market_sentiment',\n",
       "                                                   'recession_expectation',\n",
       "                                                   'growing_sector',\n",
       "                                                   'investor_type',\n",
       "                                                   'news_impact',\n",
       "                                                   'policy_uncertainty']),\n",
       "                                                 ('num', StandardScaler(),\n",
       "                                                  ['SMA_5', 'SMA_10', 'EWMA_5',\n",
       "                                                   'EWMA_10', 'Volatility_10',\n",
       "                                                   'RSI_14', 'BB_Upper',\n",
       "                                                   'BB_Lower', 'Volume_change',\n",
       "                                                   'Momentum_5',\n",
       "                                                   'log_return'])])),\n",
       "                ('smote', SMOTE(random_state=42)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(random_state=42, solver='liblinear'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1dc485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       143\n",
      "           1       0.98      0.98      0.98       194\n",
      "\n",
      "    accuracy                           0.98       337\n",
      "   macro avg       0.98      0.98      0.98       337\n",
      "weighted avg       0.98      0.98      0.98       337\n",
      "\n",
      "Confusion Matrix:\n",
      " [[139   4]\n",
      " [  4 190]]\n",
      "\n",
      "=== Test Set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95        62\n",
      "           1       0.95      0.98      0.96        83\n",
      "\n",
      "    accuracy                           0.96       145\n",
      "   macro avg       0.96      0.96      0.96       145\n",
      "weighted avg       0.96      0.96      0.96       145\n",
      "\n",
      "Confusion Matrix:\n",
      " [[58  4]\n",
      " [ 2 81]]\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de entrenamiento\n",
    "print(\"=== Training Set ===\")\n",
    "print(classification_report(y_train, pipeline1.predict(X_train)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, pipeline1.predict(X_train)))\n",
    "\n",
    "# Conjunto de prueba\n",
    "print(\"\\n=== Test Set ===\")\n",
    "print(classification_report(y_test, pipeline1.predict(X_test)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pipeline1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becca4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas – Training Set ===\n",
      "              precision    recall  f1-score  support\n",
      "0              0.972028  0.972028  0.972028    143.0\n",
      "1              0.979381  0.979381  0.979381    194.0\n",
      "accuracy       0.976261  0.976261  0.976261    337.0\n",
      "macro avg      0.975705  0.975705  0.975705    337.0\n",
      "weighted avg   0.976261  0.976261  0.976261    337.0\n",
      "\n",
      "=== Métricas – Test Set ===\n",
      "              precision    recall  f1-score  support\n",
      "0              0.966667  0.935484  0.950820     62.0\n",
      "1              0.952941  0.975904  0.964286     83.0\n",
      "accuracy       0.958621  0.958621  0.958621    145.0\n",
      "macro avg      0.959804  0.955694  0.957553    145.0\n",
      "weighted avg   0.958810  0.958621  0.958528    145.0\n",
      "\n",
      "=== Confusion Matrix: Training Set ===\n",
      "[[139   4]\n",
      " [  4 190]]\n",
      "\n",
      "=== Confusion Matrix: Test Set ===\n",
      "[[58  4]\n",
      " [ 2 81]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Generar reportes (output_dict=True) para extraer métricas\n",
    "report_train = classification_report(y_train, pipeline1.predict(X_train), output_dict=True)\n",
    "report_test  = classification_report(y_test,  pipeline1.predict(X_test),  output_dict=True)\n",
    "\n",
    "# 2) Convertir a DataFrame y seleccionar columnas relevantes\n",
    "df_train_metrics = pd.DataFrame(report_train).T[['precision','recall','f1-score','support']]\n",
    "df_test_metrics  = pd.DataFrame(report_test).T[['precision','recall','f1-score','support']]\n",
    "\n",
    "# 3) Añadir la accuracy general como fila adicional\n",
    "acc_train = accuracy_score(y_train, pipeline1.predict(X_train))\n",
    "acc_test  = accuracy_score(y_test,  pipeline1.predict(X_test))\n",
    "df_train_metrics.loc['accuracy'] = [acc_train, acc_train, acc_train, y_train.shape[0]]\n",
    "df_test_metrics.loc['accuracy']   = [acc_test,  acc_test,  acc_test,  y_test.shape[0]]\n",
    "\n",
    "# 4) Mostrar tablas de métricas\n",
    "print(\"=== Métricas – Training Set ===\")\n",
    "print(df_train_metrics)\n",
    "print(\"\\n=== Métricas – Test Set ===\")\n",
    "print(df_test_metrics)\n",
    "\n",
    "# 5) Mostrar matrices de confusión\n",
    "print(\"\\n=== Confusion Matrix: Training Set ===\")\n",
    "print(confusion_matrix(y_train, pipeline1.predict(X_train)))\n",
    "print(\"\\n=== Confusion Matrix: Test Set ===\")\n",
    "print(confusion_matrix(y_test, pipeline1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dae4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Indicadores Técnicos – Training Set ===\n",
      "                     mean        std         min         max\n",
      "SMA_5          517.654802  61.842703  406.781274  609.013953\n",
      "SMA_10         516.613157  61.568761  410.683267  605.662036\n",
      "EWMA_5         517.644265  61.749945  407.885763  608.307475\n",
      "EWMA_10        516.670783  61.488193  411.328000  606.308464\n",
      "Volatility_10    5.695952   3.421980    1.111150   26.172231\n",
      "RSI_14          58.323585  17.748074   19.366852   96.715886\n",
      "BB_Upper       531.220184  63.836143  429.865604  624.685912\n",
      "BB_Lower       498.279777  59.765673  397.083077  595.254028\n",
      "Volume_change    0.054803   0.325011   -0.541542    1.163652\n",
      "Momentum_5       1.874686  10.689483  -50.379974   29.859985\n",
      "log_return       0.000209   0.010036   -0.060327    0.032513\n",
      "\n",
      "=== Indicadores Técnicos – Test Set ===\n",
      "                     mean        std         min         max\n",
      "SMA_5          529.418059  53.188231  413.583289  606.094739\n",
      "SMA_10         528.984138  53.299482  412.431314  604.671021\n",
      "EWMA_5         529.412520  53.160577  414.035397  606.733948\n",
      "EWMA_10        528.850612  53.082827  416.032075  604.636886\n",
      "Volatility_10    6.451810   4.789220    1.562946   28.974600\n",
      "RSI_14          56.573211  15.725993   18.659854   88.167206\n",
      "BB_Upper       545.455789  56.514440  429.739544  624.118966\n",
      "BB_Lower       509.789458  51.870459  398.870346  595.555862\n",
      "Volume_change    0.028394   0.332619   -0.499327    1.172981\n",
      "Momentum_5       1.162718  13.189683  -64.489960   41.129974\n",
      "log_return       0.001566   0.011426   -0.024091    0.099863\n"
     ]
    }
   ],
   "source": [
    "# Descripción en el conjunto de entrenamiento\n",
    "desc_train = X_train[numerical_features].describe().T[['mean','std','min','max']]\n",
    "print(\"=== Indicadores Técnicos – Training Set ===\")\n",
    "print(desc_train)\n",
    "\n",
    "# Descripción en el conjunto de prueba\n",
    "desc_test  = X_test[numerical_features].describe().T[['mean','std','min','max']]\n",
    "print(\"\\n=== Indicadores Técnicos – Test Set ===\")\n",
    "print(desc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2019f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de componentes para 80% de varianza: 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "pca_full = PCA().fit(X_train_proc)\n",
    "cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_80 = int(np.argmax(cum_var >= 0.8) + 1)\n",
    "print(f\"Número de componentes para 80% de varianza: {n_components_80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b4e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline  # o sklearn.pipeline.Pipeline si no usas SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pipeline2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA()),               # n_components lo pondremos en el grid\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(solver='liblinear', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f4502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [n_components_80],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline2,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2173db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Mejores parámetros: {'classifier__C': 10, 'pca__n_components': 8}\n",
      "Mejor F1-score CV: 0.8910476190476191\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor F1-score CV:\", grid_search.best_score_)\n",
    "\n",
    "best_model2 = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1559b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Part 2: Training Set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       143\n",
      "           1       0.91      0.89      0.90       194\n",
      "\n",
      "    accuracy                           0.88       337\n",
      "   macro avg       0.88      0.88      0.88       337\n",
      "weighted avg       0.88      0.88      0.88       337\n",
      "\n",
      "Confusion Matrix:\n",
      " [[125  18]\n",
      " [ 22 172]]\n",
      "\n",
      "=== Part 2: Test Set ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85        62\n",
      "           1       0.88      0.90      0.89        83\n",
      "\n",
      "    accuracy                           0.88       145\n",
      "   macro avg       0.87      0.87      0.87       145\n",
      "weighted avg       0.88      0.88      0.88       145\n",
      "\n",
      "Confusion Matrix:\n",
      " [[52 10]\n",
      " [ 8 75]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"=== Part 2: Training Set ===\")\n",
    "print(classification_report(y_train, best_model2.predict(X_train)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, best_model2.predict(X_train)))\n",
    "\n",
    "print(\"\\n=== Part 2: Test Set ===\")\n",
    "print(classification_report(y_test, best_model2.predict(X_test)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, best_model2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59947e5",
   "metadata": {},
   "source": [
    "### 3.1 Impacto de falsos positivos (FP) y falsos negativos (FN)\n",
    "\n",
    "- **Falsos positivos (FP)**  \n",
    "  El modelo predice “Buy” (1) cuando en realidad el siguiente retorno es negativo.  \n",
    "  - Consecuencias:  \n",
    "    - Se abre una posición perdedora.  \n",
    "    - Se incurren costes de transacción.  \n",
    "    - En entornos apalancados, un FP puede disparar drawdowns significativos.  \n",
    "\n",
    "- **Falsos negativos (FN)**  \n",
    "  El modelo predice “Do Not Buy” (0) cuando el retorno real iba a ser positivo.  \n",
    "  - Consecuencias:  \n",
    "    - Se pierden oportunidades de ganancia.  \n",
    "    - Reduce el rendimiento global y la eficiencia del capital.  \n",
    "    - En mercados con fuerte tendencia alcista, implica baja participación en la subida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5093d4",
   "metadata": {},
   "source": [
    "### 3.2 ¿Qué métrica debe importar más a la firma?\n",
    "\n",
    "La elección depende de la relación pérdida/ganancia y de la tolerancia al riesgo:\n",
    "\n",
    "| Métrica      | Definición                         | ¿Cuándo priorizarla?                                                                 |\n",
    "|--------------|------------------------------------|---------------------------------------------------------------------------------------|\n",
    "| **Precision**| TP / (TP + FP)                     | Cuando un FP es muy caro (costes altos, apalancamiento).                              |\n",
    "| **Recall**   | TP / (TP + FN)                     | Cuando perder oportunidades de ganancia penaliza fuertemente el alpha.                |\n",
    "| **F1-score** | 2·(Precision·Recall)/(Precision+Recall) | Cuando se desea un compromiso equilibrado entre FP y FN.                              |\n",
    "\n",
    "- **Priorizar Precision**: si el coste de un FP supera con creces la ganancia de un FN.  \n",
    "- **Priorizar Recall**: si maximizar capture trades rentable es crítico.  \n",
    "- **F1-score**: buena métrica intermedia para balancear ambos tipos de error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4d396",
   "metadata": {},
   "source": [
    "### 3.3 Conclusión: ventajas de ML sobre solo indicadores técnicos\n",
    "\n",
    "1. **Aprendizaje de patrones complejos**  \n",
    "   Combina múltiples indicadores y variables exógenas para capturar relaciones no lineales.  \n",
    "2. **Adaptabilidad**  \n",
    "   Reentrenas el modelo con datos nuevos para adaptarte a cambios de régimen sin recalibrar manualmente.  \n",
    "3. **Interpretabilidad cuantitativa**  \n",
    "   La regresión logística ofrece coeficientes que cuantifican el peso de cada indicador en la probabilidad de subida.  \n",
    "4. **Backtesting y robustez**  \n",
    "   Validación cruzada y SMOTE mejoran la generalización y reducen overfitting.\n",
    "\n",
    "En definitiva, una ML Pipeline potencia tus indicadores técnicos, aportando señales más precisas, adaptables y respaldadas por métricas objetivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d7355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
